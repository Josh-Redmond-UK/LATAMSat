{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDQEyEj4apxC",
    "outputId": "483ed313-1148-4883-ed12-ffd514b61daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshredmond/miniconda3/envs/tensorflowMac/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a7WhOzURLGFA"
   },
   "outputs": [],
   "source": [
    "def get_lulc_class(path):\n",
    "  splits = path.split('/')\n",
    "  return splits[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tmfApOIMT4XS"
   },
   "outputs": [],
   "source": [
    "def multispectral_to_rgb(raster, optical_maximum = 2000):\n",
    "\n",
    "  r = raster[:, :, 3]\n",
    "  g = raster[:, :, 2]\n",
    "  b = raster[:, :, 1]\n",
    "\n",
    "  rgb_raster = np.stack([r, g, b], axis=2)\n",
    "\n",
    "  #cast to uint and scale to 0/255\n",
    "\n",
    "  rgb_raster = rgb_raster/optical_maximum\n",
    "  rgb_raster = np.around(rgb_raster*255)\n",
    "  rgb_raster = np.clip(rgb_raster, 0, 255).astype(int)\n",
    "  return rgb_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Zdh1hcFLVYm-"
   },
   "outputs": [],
   "source": [
    "def rescale_image(raster):\n",
    "  max_val = np.max(raster)\n",
    "  mid_val = max_val/2\n",
    "  rescaled = (raster-mid_val)/(mid_val)\n",
    "  return np.clip(rescaled, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "09GjC2UXWAv6"
   },
   "outputs": [],
   "source": [
    "def rio_to_channels_last(raster):\n",
    "  return raster.transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6UylQeatXsWU"
   },
   "outputs": [],
   "source": [
    "def get_array(path):\n",
    "  _r = tifffile.imread(path)\n",
    "  arr = rio_to_channels_last(_r)\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "piJ9G82aYzRR"
   },
   "outputs": [],
   "source": [
    "def get_image_paths(top_level_path):\n",
    "  ecoregion_folders = glob.glob(top_level_path+'/*')\n",
    "  img_paths = []\n",
    "  for ec_dir in ecoregion_folders:\n",
    "    img_paths += glob.glob(ec_dir+'*/*.tif')\n",
    "  return img_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iACm5bKtEkpN"
   },
   "outputs": [],
   "source": [
    "class latamSatGenerator():\n",
    "  def __init__(self, top_level_path, batch_size=32):\n",
    "    print('generating paths')\n",
    "    self.top_level_path = top_level_path\n",
    "    img_paths = get_image_paths(self.top_level_path)\n",
    "    random.shuffle(img_paths)\n",
    "    self.img_paths = img_paths\n",
    "    self.batch_size = batch_size\n",
    "    self.img_classes = np.unique(np.array([get_lulc_class(i) for i in self.img_paths]))\n",
    "\n",
    "  def random_image_generator(self, supervised=True, seed=1, rgb=True, normalise=False, numpy=False):\n",
    "    #get image paths\n",
    "    img_paths =self.img_paths\n",
    "    batch_size = self.batch_size\n",
    "\n",
    "    #randomly sample a batch\n",
    "    num_batches = (len(img_paths) // batch_size) - 1\n",
    "    #get arrays according to params\n",
    "    for _b in range(num_batches):\n",
    "      arrays = []\n",
    "      classifications = []\n",
    "      batch = img_paths[_b*batch_size:(_b+1)*batch_size]\n",
    "      for img in batch:\n",
    "        _arr = get_array(img)\n",
    "        if rgb:\n",
    "          _arr = multispectral_to_rgb(_arr)\n",
    "\n",
    "        if normalise:\n",
    "          _arr = rescale_image(_arr)\n",
    "\n",
    "\n",
    "        arrays.append(_arr)\n",
    "        if supervised:\n",
    "          class_idx = get_lulc_class(img)\n",
    "          one_hot = tf.one_hot(np.where(dataset.img_classes == class_idx)[0][0], len(dataset.img_classes))\n",
    "          classifications.append(one_hot)\n",
    "\n",
    "\n",
    "      if supervised:\n",
    "        yield np.squeeze(np.array(arrays)), np.squeeze(np.array(classifications))\n",
    "      else:\n",
    "        yield np.squeeze(np.array(arrays))\n",
    "\n",
    "\n",
    "  def make_tf_dataset(self, rgb=True, supervised=True, normalise=False, seed=1):\n",
    "    if rgb:\n",
    "      if normalise:\n",
    "        img_sig = tf.TensorSpec(shape=(1, 64,64,3), dtype=tf.float32)\n",
    "      else:\n",
    "        img_sig = tf.TensorSpec(shape=(1, 64,64,3), dtype=tf.int32)\n",
    "\n",
    "    else:\n",
    "      if normalise:\n",
    "        img_sig = tf.TensorSpec(shape=(1, 64,64,13), dtype=tf.float32)\n",
    "      else:\n",
    "        img_sig = tf.TensorSpec(shape=(1, 64,64,13), dtype=tf.int32)\n",
    "\n",
    "\n",
    "    if supervised:\n",
    "      class_sig = tf.TensorSpec(shape=((1, len(self.img_classes))), dtype=tf.float32)\n",
    "      output_sig = (img_sig, class_sig)\n",
    "    else:\n",
    "      output_sig = (img_sig)\n",
    "\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(lambda: self.random_image_generator(rgb=rgb, supervised=supervised, normalise=normalise, seed=seed),\n",
    "                                                 output_signature=output_sig)\n",
    "    return img_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wHHW6th6aJRr"
   },
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, batch_size=64, shuffle_buffer_size=1000):\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "  # shuffle the dataset\n",
    "  #ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "  # Repeat forever\n",
    "  #ds = ds.repeat()\n",
    "  # split to batches\n",
    "  ds = ds.batch(batch_size)\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregion_paths = 'latamSatData/DownloadedDataset/*'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GY5wdB73GJY0",
    "outputId": "e1465613-1cfd-4b7d-a3d6-1b2819f533a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating paths\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = latamSatGenerator(ecoregion_paths, batch_size=64)\n",
    "img_generator = dataset.random_image_generator(normalise=True)\n",
    "#img_generator = dataset.make_tf_dataset(normalise=True, supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latamSatData/DownloadedDataset/*'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecoregion_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1dRIUdk5N4UT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 12:08:42.243389: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2023-09-15 12:08:42.243442: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-09-15 12:08:42.243456: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-09-15 12:08:42.243543: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-15 12:08:42.243588: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "testxy = next(img_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0ch81ZHQckD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3dZS7C6P6Fq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Uq6hozOham5A"
   },
   "outputs": [],
   "source": [
    "#train_ds = prepare_for_training(img_generator, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnWADBhaiBUz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qwV58-j7SnSB"
   },
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\"\n",
    "\n",
    "# download & load the layer as a feature vector\n",
    "keras_layer = hub.KerasLayer(model_url, output_shape=[1280], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFMYdCBROJqL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "md86UUYaVcqu"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.Sequential([\n",
    "  keras_layer,\n",
    "  tf.keras.layers.Dense(len(dataset.img_classes), activation=\"softmax\")\n",
    "])\n",
    "# build the model with input image shape as (64, 64, 3)\n",
    "m.build([None, 64, 64, 3])\n",
    "m.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "REYzLMQjW8-I"
   },
   "outputs": [],
   "source": [
    "model_name = \"satellite-classification\"\n",
    "model_path = os.path.join( model_name + \".h5\")\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "uxd2GAcJXDPy",
    "outputId": "a2826415-34ca-43c2-b274-1cbe577cf298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 12:19:08.908994: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/500 [..............................] - ETA: 18:08 - loss: 2.7851 - accuracy: 0.1652"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = m.fit(\n",
    "    img_generator,\n",
    "    verbose=1, epochs=5,\n",
    "    steps_per_epoch=500,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
